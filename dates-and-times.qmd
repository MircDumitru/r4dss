# Dates and times {#sec-dates-and-times}

```{r}
#| echo: false

source("_settings.R")
```

## Introduction

This chapter will show you how to work with dates and times in R. At first glance, dates and times seem simple. You use them all the time in your regular life, and they don't seem to cause much confusion. However, the more you learn about dates and times, the more complicated they seem to get.

To warm up think about how many days there are in a year and how many hours are there in a day. You probably remembered that most years have 365 days, but leap years have 366. Do you know the full rule for determining if a year is a leap year[^dates-and-times-1]? The number of hours in a day is a little less obvious: most days have 24 hours, but in place that use daylight saving time (DST), one day each year has 23 hours and another 25.

[^dates-and-times-1]: A year is a leap year if it's divisible by for, unless it's also divisible by 100, except if it's also divisible by 400. In other words, in every set of 400 years, there's 97 leap years.

Dates and times are hard because they have to reconcile two physical phenomena (the rotation of the Earth and its orbit around the sun) with a whole raft of geopolitical phenomena including months, time zones and DST. This chapter won't teach you every last detail about dates and times, but it will give you a solid grounding of practical skills that will help you with common data analysis challenges.

We'll begin the showing you how to create date-times from various inputs, and then once you've got a date-time, how you can extract components like year, month, and day. We'll then dive into the tricky topic of working with time spans, which come in a variety of flavors depending on what you're trying to do. We'll conclude with a brief discussion of the additional challenges posed by time zones.

### Prerequisites

This chapter will focus on the **lubridate** package, which makes it easier to work with dates and times in R. As of the latest tidyverse release, lubridate is part of core tidyverse. We will also need nycflights13 for practice data.

```{r}
#| warning: false
library(tidyverse)
library(nycflights13)
```

## Creating date/times

There are three types of date/time data that refer to an instant in time:

-   A **date**. Tibbles print this as `<date>`.

-   A **time** within a day. Tibbles print this as `<time>`.

-   A **date-time** is a date plus a time: it uniquely identifies an instant in time (typically to the nearest second). Tibbles print this as <dttm>. Base R calls these POSIXct.

In this chapter we are going to focus on dates and date-times as R doesn't have a native class for storing times. If you need one, you can use the **hms** package.

You should always use the simplest possible data type that works for your needs. That means if you can use a date instead of a date-time, you should. Date-times are substantially more complicated because of the need to handle time zones, which we'll come back to at the end of the chapter.

To get the current date or date-time you can use `today()` or `now()`:

```{r}
today()
now()
```

Otherwise, the following sections describe the four ways you're likely to create a date/time:

-   While reading a file with readr.
-   From a string.
-   From individual date-time components.
-   From an existing date/time object.

### During import

If your CSV contains an ISO8601 date or date-time, you don't need to do anything; readr will automatically recognize it:

```{r}
#| message: false
csv <- "
date, datetime
2022-01-02,2022-01-02 04:34
"
read_csv(csv)
```

If you haven't heard of **ISO8601** before, it's an international standard for writing dates where the components of a date are organized from biggest to smallest separated by `-`. For example, in ISO8601 May 3 2022 is `2022-05-03`. ISO8601 dates can also include times, where hour, minute and second are separated by `:`, and the date and time components are separated by either a `T` or a space. For example you could write 4:26pm on May 3 2022 as either `2022-05-03 16:26` or `2022-05-03T16:26`.

For other date-times formats, you'll need to use `col_types` plus `col_date()` or `col_datetime()` along with a date-time format. The date-time format used by readr is a standard used across many programming languages, describing a date component with a `%` followed by a single character. For example `%Y-%m-%d` specifies a date that's a year, `-`, month (as number) `-`, day. Table @tbl-date-formats lists all the options.

| Type  | Code  |            Meaning             |     Example     |
|:-----:|:-----:|:------------------------------:|:---------------:|
| Year  | `%Y`  |          4 digit year          |      2021       |
|       | `%y`  |          2 digit year          |       21        |
| Month | `%m`  |             Number             |        2        |
|       | `%b`  |        Abbreviated name        |       Feb       |
|       | `%B`  |           Full name            |    February     |
|  Day  | `%d`  |       One or two digits        |        2        |
|       | `%e`  |           Two digits           |       02        |
| Time  | `%H`  |          24-hour hour          |       13        |
|       | `%I`  |          12-hour hour          |        1        |
|       | `%p`  |             AM/PM              |       pm        |
|       | `%M`  |            Minutes             |       35        |
|       | `%S`  |            Seconds             |       45        |
|       | `%OS` | Seconds with decimal component |      45.35      |
|       | `%Z`  |         Time zone name         | America/Chicago |
|       | `%z`  |        Offset from UTC         |      +0800      |
| Other | `%.`  |       Skip one non-digit       |        :        |
|       | `%*`  | Skip any number of non-digits  |                 |

: All date formats understood by readr {#tbl-date-formats}

The following code shows a few options applied to a very ambiguous date:

```{r}
csv <- "
date1,date2
01/02/15,12:04:11
"
read_csv(csv, col_types = cols(date1 = col_date("%m/%d/%y"),
                               date2 = col_date("%y:%m:%d")))

read_csv(csv, col_types = cols(date1 = col_date("%d/%m/%y"),
                               date2 = col_date("%d:%m:%y")))

read_csv(csv, col_types = cols(date1 = col_date("%y/%m/%d"),
                               date2 = col_date("%d:%y:%m")))
```

Note that no matter how you specify the date format, it's always displayed the same way once you get it into R.

If you're using `%b` or `%B` and working with non-English dates, you'll also need to provide a `locale()`. See the list of built-in languages in `date_names_langs()` or create your own with `date_names()`.

### From strings

The date-time specification language is powerful, but requires careful analysis of the date format. An alternative approach is to use `lubridate`'s helpers which attempt to automatically determine the format once you specify the order of the component. To use them, identify the order in which year, month, and day appear in your dates, then arrange "y", "m", and "d" in the same order. That gives you the name of the `lubridate` function that will parse your date. For example:

```{r}
ymd("2017-01-31")
mdy("January 31st, 2017")
dmy("31-Jan-2017")
```

`ymd()` and friends create dates. To create a date-time, add an underscore and one or more of "h", "m", and "s" to the name of the parsing function:

```{r}
ymd_hms("2017-01-31 20:11:43")
mdy_hm("01/31/2017 08:03")
```

You can also force the creation of a date-time from a date by supplying a timezone:

```{r}
ymd("2017-01-31", tz = "UTC")
```

Here UTC timezone are used which you might also know as GMT, or Greenwich Mean Time, the time at 0Â° longitude. It doesn't use daylight saving time, making it a bit easier to compute with.

### From individual components

Instead of a single string, sometimes you'll have the individual components of the date-time spread across multiple columns. This is what we have in the `flights` data:

```{r}
flights |>
  select(year, month, day, hour, minute)
```

To create a date/time from this sort of input, use `make_date()` for dates or `make_datetime()` for date-times:

```{r}
flights |>
  select(year, month, day, hour, minute) |>
  mutate(departure = make_datetime(year, month, day, hour, minute))
```

Let's do the same thing for each of the four columns in `flights`. The time are represented in a slightly odd format, so we use modulus arithmetic to pull out the hour and minute components. Once we've created the date-time variables, we focus on the variables we'll explore in the rest of the chapter.

```{r}
make_datetime_100 <- function(year, month, day, time) {
  make_datetime(year, month, day, time %/% 100, time %% 100)
}

flights_dt <- flights |>
  filter(!is.na(dep_time), !is.na(arr_time)) |>
  mutate(
    dep_time = make_datetime_100(year, month, day, dep_time),
    arr_time = make_datetime_100(year, month, day, arr_time),
    sched_dep_time = make_datetime_100(year, month, day, sched_dep_time),
    sched_arr_time = make_datetime_100(year, month, day, sched_arr_time)
  ) |>
  select(origin, dest, ends_with("delay"), ends_with("time"))

flights_dt
```

With this data, we can visualize the distribution of departure times across the year:

```{r}
flights_dt |>
  ggplot(aes(x = dep_time)) +
  geom_freqpoly(binwidth = 86400) # 86400 seconds = 1 day
```

Or within a single day:

```{r}
flights_dt |>
  filter(dep_time < ymd(20130102)) |>
  ggplot(aes(x = dep_time)) +
  geom_freqpoly(binwidth = 600) # 600 seconds = 10 minutes 
```

Note that when you use date-times in a numeric context (like a histogram) 1 means 1 second, so a binwidth of 86400 means one day. For dates, 1 means 1 day.

### From other types

You may want to switch between a date-time a date. That's the job of `as_datetime()` and `as_date()`:

```{r}
today()
as_datetime(today())
now()
as_date(now())
```

Sometimes you'll get date/times as numeric offsets from the "Unix Epoch", 1970-01-01. If the offset is in seconds, use `as_datetime()`; if it's in days, use `as_date()`.

```{r}
as_datetime(60 * 60 * 10)
as_date(365 * 10 + 2)
```

### Exercises

1.  What happens if you parse a string that contains invalid dates?

    ```{r}
    ymd(c("2010-10-10", "bananas"))
    ```

    When parsing strings that contain invalid dates a warning is thrown and the string is parsed to `NA`.

2.  What does the `tzone` argument to `today()` do? Why is it important?

    Both `now()` and `today()` functions are having as argument `tzone`, with the default value `""` i.e. `now(tzone = "")` and `today(tzone = "")`. The default argument defaults to the computer's system timezone. We can retrieve the current time in UTC using `tzone = "UTC"` or other time systems.

    ```{r}
    today()
    today(tzone = "NZ")
    now()
    now(tzone = "NZ")
    ```

3.  For each of the following date-times, show how you'd parse it using a readr column specification and a lubridate function.

    For `d1`, the `mdy` lubridate function is able to parse the string. For the column specification, the only tricky step is the construction of the csv file: since `d1` contains a comma, the string iteself has to be written in quotation marks. The column specification is `col_date("%B %d, %Y")`.

    ```{r}
    d1 <- "January 1, 2010"
    mdy(d1)
    csv <- str_flatten(c("d1\n\"", d1, "\""))
    # csv
    # str_view(csv)
    read_csv(csv, col_types = cols(d1 = col_date("%B %d, %Y")))
    ```

    For `d2`, the `ymd` lubridate function is able to parse the string. The column specification is `col_date("%Y-%b-%d")`.

    ```{r}
    d2 <- "2015-Mar-07"
    ymd(d2)
    csv <- str_flatten(c("d2", d2), "\n")
    # csv
    # str_view(csv)
    read_csv(csv, col_types = cols(d2 = col_date("%Y-%b-%d")))
    ```

    For `d3`, the `dmy` lubridate function is able to parse the string. The column specification is `col_date("%d-%b-%Y")`.

    ```{r}
    d3 <- "06-Jun-2017"
    dmy(d3)
    csv <- str_flatten(c("d3", d3), "\n")
    # csv
    # str_view(csv)
    read_csv(csv, col_types = cols(d3 = col_date("%d-%b-%Y")))
    ```

    For `d4`, the `mdy` lubridate function is able to parse the string. The column specification is `col_date("%B %d (%Y)")`.

    ```{r}
    d4 <- c("August 19 (2015)", "July 1 (2015)")
    mdy(d4)
    csv <- str_flatten(c("d4", d4), "\n")
    # csv
    # str_view(csv)
    read_csv(csv, col_types = cols(d4 = col_date("%B %d (%Y)")))
    ```

    For `d5`, the `mdy` lubridate function is able to parse the string. The column specification is `col_date("%m/%d/%y")`.

    ```{r}
    d5 <- "12/30/14" # Dec 30, 2014
    mdy(d5)
    csv <- str_flatten(c("d5", d5), "\n")
    # csv
    # str_view(csv)
    read_csv(csv, col_types = cols(d5 = col_date("%m/%d/%y")))
    ```

    For `t1`, the `hm` lubridate function is not able to parse the string. In order to parse it, first substrings are extracted and then flatten, inserting `:` (this could have been any value like `-`, `/`, etc), so `hm` can work. For `t1` I don't think without some modification on the string any `lubridate` function can parse it. The column specification is `col_time("%H%M")`.

    ```{r}
    t1 <- "1705"
    t12 <- str_sub(t1, seq(1, str_count(t1) - 1, 2), seq(2, str_count(t1), 2))
    t12 <- str_flatten(t12, ":")
    hm(t12)
    csv <- str_flatten(c("t1", t1), "\n")
    # csv
    # str_view(csv)
    read_csv(csv, col_types = cols(t1 = col_time("%H%M")))
    ```

    For `t2`, the `hms` lubridate function is able to parse the string. The column specification is `col_time("%I:%M:%OS %p")`

    ```{r}
    t2 <- "11:15:10.12 PM"
    hms(t2)
    csv <- str_flatten(c("t2", t2), "\n")
    # csv
    # str_view(csv)
    read_csv(csv, col_types = cols(t2 = col_time("%I:%M:%OS %p")))
    ```

## Date-time components

Now that you know how to get date-time into R's data-time data structures, let's explore what you can do with them. This section will focus on the accessor functions that let you get and set individual components. The next section will look at how arithmetic works with date-times.

### Getting components

You can pull out individual parts of the date with the accessor functions `year()`, `month()`, `mday()` (day of the month) `yday()` (day of the year), `wday()` (day of the week), `hour()`, `minute()`, and `second()`. These are effectively the opposites of `make_datetime()`.

```{r}
datetime <- ymd_hms("2026-07-08 12:34:56")

year(datetime)
month(datetime)
mday(datetime)
yday(datetime)
wday(datetime)
```

For `month()` and `wday()` you can set `label = TRUE` to return the abbreviated name of the month or day of the week. Set `abbr = FALSE` to return the full name.

```{r}
month(datetime, label = TRUE)
wday(datetime, label = TRUE, abbr = FALSE)
```

We can use `wday()` to see that more flights depart during the week than on the weekend:

```{r}
flights_dt |>
  mutate(wday = wday(dep_time, label = TRUE)) |>
  ggplot(aes(x = wday)) +
  geom_bar()
```

We can also look at the average departure delay by minute within the hour. There's an interesting pattern: flight leaving in minutes 20-30 and 50-60 have much lower delays than the rest of the hour!

```{r}
flights_dt |>
  mutate(minute = minute(dep_time)) |>
  group_by(minute) |>
  summarise(
    avg_delay = mean(dep_delay, na.rm = TRUE),
    n = n()
  ) |>
  ggplot(aes(x = minute, y = avg_delay)) +
  geom_line()
```

Interestingly, if we look at the *scheduled* departure time we don't see such a strong pattern:

```{r}
sched_dep <- flights_dt |>
  mutate(minute = minute(sched_dep_time)) |>
  group_by(minute) |>
  summarise(
    avg_delay = mean(arr_delay, na.rm = TRUE),
    n = n()
  )

sched_dep |>
  ggplot(aes(x = minute, y = avg_delay)) +
  geom_line()
```

So why do we see that pattern with the actual departure time? Well, like much data collected by humans, there is a strong bias towards flights leaving at "nice" departure times, as @fig-departure-times shows. Always be alert for this sort of pattern whenever you work with data that involves human judgement.

```{r}
#| label: fig-departure-times
#| fig-cap: |
#|  Plot showing the number of lfights scheduled to depart each hour. 
#|  There is a strong preference for round numbers like 0 and 30 
#|  generally for numbers that are a multiple of five

sched_dep |>
  ggplot(aes(x = minute, y = n)) +
  geom_line()
```

### Rounding

An alternative approach to plotting individual components is to round the date to a nearby unit of time, with `floor_date()`, `round_date()` and `ceiling_date()`. Each function takes a vector of dates to adjust and then the name of the unit to round down (floor), round up (ceiling), or round to. This, for example, allows us to plot the number of flights per week:

```{r}
flights_dt |>
  count(week = floor_date(dep_time, "week")) |>
  ggplot(aes(x = week, y = n)) +
  geom_line() +
  geom_point()
```

You can use rounding to show the distribution of flights across the course of a day by computing the difference between `dep_time` and the earliest instant of that day:

```{r}
flights_dt |>
  mutate(dep_hour = dep_time - floor_date(dep_time, "day")) |>
  ggplot(aes(x = dep_hour)) +
  geom_freqpoly(binwidth = 60 * 30)
```

Computing the difference between a pair of date-times yields a difftime (more on that in @sec-time-spans. We can convert that to an hms object to a more useful x-axis:

```{r}
flights_dt |>
  mutate(dep_hour = hms::as_hms(dep_time - floor_date(dep_time, "day"))) |>
  ggplot(aes(x = dep_hour)) +
  geom_freqpoly(binwidth = 60 * 30)
```

### Modifying components

You can also use each accessor function to modify the components of a date/time. This doesn't come up much in data analysis, but can be useful when cleaning data that has clearly incorrect dates.

```{r}
(datetime <- ymd_hms("2026-07-08 12:34:56"))

year(datetime) <- 2030
datetime
month(datetime) <- 01
datetime
hour(datetime) <- hour(datetime) + 1
datetime
```

Alternatively, rather than modifying an existing variable, you can create a new date-time with `update()`. This also allows you to set multiple values in one step:

```{r}
update(datetime, year = 2030, month = 2, mday = 2, hour = 2)
```

If values are too big, they will roll-over:

```{r}
update(ymd("2025-02-01"), mday = 30)
update(ymd("2025-02-01"), hour = 400)
```

### Exercises

1.  How does the distribution of flight times within a day change over the course of the year?

    ```{r}
    #| layout-ncol: 2
    #| fig-width: 4

    dep_df <- flights_dt |>
      select(dep_time) |>
      mutate(
        dep_date = date(dep_time),
        dep_hour = hms::as_hms(dep_time)
        ) 

    dep_df |>
      ggplot(aes(x = hour(dep_hour), group = dep_date)) +
      geom_density() +
      labs(
        title = "Distribution of dep. hour within a day over a year (density)",
      )

    dep_df |>
      ggplot(aes(x = dep_hour, group = dep_date)) +
      geom_freqpoly(binwidth = 3600, alpha = .3) + 
      labs(
        title = "Distribution of dep. hour within a day over a year (freqpoly)",
      )
    ```

2.  Compare `dep_time`, `sched_dep_time` and `dep_delay`. Are they consistent? Explain your findings.

    In order for `dep_time`, `sched_dep_time` and `dep_delay` to be consistent we would expect that `dep_time - sched_dep_time` to be equal to `dep_delay`. We can easily test by first mutating the `flights_dt` tibble and adding column checking for the equality between `dep_time - sched_dep_time` and `dep_delay`. Since `dep_time`, `sched_dep_time` are date-time formats, taking the difference between them is sensible. Since `dep_delay` is a double, representing the departure delay in minutes, we need to transform `dep_time - sched_dep_time` to double as well and transform seconds in minutes by dividing by 60. Then we can check if the variable checking for this equality has only `TRUE` valus, i.e. if the equality holds for all observations:

    ```{r}
    flights_const_check <- flights_dt |>
      select(dep_time, sched_dep_time, dep_delay) |>
      mutate(
        check = as.numeric((dep_time - sched_dep_time) / 60) == dep_delay
        )
      
    all(flights_const_check$check)
    ```

    We notice that in fact the three variables are not consistent for all observations. We can check the observations where the equality does not hold by filtering the mutated tibble by the values where the equality does not hold:

    ```{r}
    flights_const_check |>
      filter(check == FALSE)
    ```

    The inconsistency comes from the method used to create `flights_dt` tibble. It was created using the function `make_datetime_100()`, which modified the numerical variables `dep_time`, `arr_time`, `sched_dep_time` and `sched_arr_time` to date-time variables based on the numerical variables `year`, `month`, `day`. The error comes from the fact that while modifying the `sched_dep_time` variable using

    ```{r}
    #| eval: false
    sched_dep_time = make_datetime_100(year, month, day, sched_dep_time)
    ```

    is correct, using the same formula for the other three variables is wrong since it doesn't account for for possible day rolling. By using

    ```{r}
    #| eval: false
    dep_time = make_datetime_100(year, month, day, dep_time)
    ```

    the date will always be the same as the departure date, which is not accounting for the observations where the departure was actually delayed enough such that it happens the next day (i.e. another date).

    For the `dep_time` and `arr_time` variables, by looking only at the numerical variables `dep_time` and `arr_time` it is impossible to determine to which date they refers, so for computing the date-time variables `dep_time`, and `arr_time`, `sched_dep_time` and `dep_delay` and `sched_arr_time` and `arr_delay`, respectively, are used:

    ```{r}
    #| eval: false
    dep_time = sched_dep_time + dep_del * 60
    arr_time = sched_arr_time + arr_del * 60
    ```

    For the `sched_arr_time` variable, the fact that there are no flights longer than 24 hours (actually the longest flight is 11 hours and 35 minutes) will be used. We can check the longest flight by looking at `airtime` variable:

    ```{r}
    longest <- flights |>
      filter(!is.na(air_time)) |>
      select(air_time) |> 
      max()
    longest <- str_c(as.character(longest %/% 60), ":", as.character(longest %% 60))
    longest <- hm(longest)
    longest
    ```

    This can be used to compute the scheduled arrival date by comparing the scheduled departure time

    ```{r}
    #| eval: false
    sched_arr_time = if_else(sched_arr_time - sched_dep_time < 0, sched_arr_time + 24 * 60 * 60, sched_arr_time)
    ```

    The following snippets correspond to the computation of `flights_dt2` and running the same test used for `flights_dt1` to check for inconsistencies between `dep_time`, `sched_dep_time` and `dep_delay` in `flights_dt2`.

    ```{r}
    flights_dt2 <- flights |>
      filter(!is.na(dep_time), !is.na(arr_time), !is.na(arr_delay)) |>
      mutate(
        sched_dep_time = make_datetime_100(year, month, day, sched_dep_time),
        dep_time = sched_dep_time + dep_delay * 60,
        sched_arr_time = make_datetime_100(year, month, day, sched_arr_time),
        sched_arr_time = if_else(sched_arr_time - sched_dep_time < 0, sched_arr_time + 24 * 60 * 60, sched_arr_time),
        arr_time = sched_arr_time + arr_delay * 60
      ) |>
      select(origin, dest, ends_with("delay"), ends_with("time"))
      
    flights_dt2
    ```

    ```{r}
    flights_const_check2 <- flights_dt2 |>
      select(dep_time, sched_dep_time, dep_delay) |>
      mutate(
        check = as.numeric((dep_time - sched_dep_time) / 60) == dep_delay
      )
      
    all(flights_const_check2$check)
    ```

    To see how the two tibbles differ, we can check the observations with discrepancies in `flights_dt1` and compare them with the observations in `flights_dt2`.

    ```{r}
    rn <- flights_const_check |>
      rownames_to_column("rn") |>
      filter(check == FALSE) |>
      pull(rn)
      
    flights_dt[rn[1:5], ] |>
      select(sched_dep_time, dep_delay, dep_time)
      
    flights_dt2[rn[1:5], ] |>
      select(sched_dep_time, dep_delay, dep_time)
    ```

3.  Compare `air_time` with the duration between the departure and arrival. Explain your findings. (Hint: consider the location of the airport.)

    ```{r}
    flights_dt2 |>
      filter(!is.na(air_time)) |>
      mutate(
        flight_time = as.numeric(arr_time - dep_time),
        difference = flight_time - air_time
        ) |>
      select(flight_time, air_time, difference) |>
      count(difference > 0) |>
      mutate(prop = n/sum(n))
    ```

    The `air_time` should always be smaller than `flight_time` and this is not true for about 40% of the flights. This is because the departure and arrival times are expressed in the local time zone, hence taking the difference between the two does not represent the time duration between departure and arrival.

    The package `nycflights13` has between its sets the `airports` dataset, which contains airport's time zones:

    ```{r}
    airports |>
      select(faa, name, tz, tzone) |>
      head()
    ```

    The common variable between `airport` and `flights` is the variable `faa` in `airport` and `origin` and `dest` in `flights` which represents the FAA airport code. The two can be joined together so that the `tzone` variable can be used so the difference between departure and arrival time is sensible.

    ```{r}
    # Selecting just the `faa` and `tzone` variables from `airports` dataset
    # in order for the joining to not include the unnecessary variables.
    airports_cut <- airports |>
      select(faa, tzone)
      
    # Joining the airports_cut to flights_tz.
    # Because the joining is done twice, corresponding to origin and dest
    # suffix are setted so we can differentiate between tzone_origin 
    # and tzone_dest.
      
    flights_tzone <- flights_dt2 |>
      left_join(airports_cut, by = c("origin" = "faa")) |>
      left_join(airports_cut, by = c("dest" = "faa"), suffix = c("_origin", "_dest"))
    ```

    The next snippet is checking the joined tibble, i.e. looking for NA values in the joined variables

    ```{r}
    flights_tzone |>
      filter(is.na(tzone_dest)) |>
      dim()
      
    na_airports <- flights_tzone |>
      filter(is.na(tzone_dest)) |>
      pull(dest) |>
      unique()
      
    na_airports
    ```

    There are four values for `dest` variable, i.e. four airport codes in the `flights` dataset that don't have a corresponding in the `airport` dataset.

    ```{r}
    airports_cut$faa[str_detect(airports_cut$faa, 
                                str_c(str_sub(na_airports[1], 1, 2), ".{1}"))]
    airports_cut$faa[str_detect(airports_cut$faa, 
                                str_c(str_sub(na_airports[2], 1, 2), ".{1}"))]
    airports_cut$faa[str_detect(airports_cut$faa, 
                                str_c(str_sub(na_airports[3], 1, 2), ".{1}"))]
    airports_cut$faa[str_detect(airports_cut$faa, 
                                str_c(str_sub(na_airports[4], 1, 2), ".{1}"))]
    ```

    For the first one it is easy to spot the type: `"BQK"` airport was entered in the `flight` dataset as `"BQN"`. But since for the other three is difficult to decide the true corresponding all the NA values will be excluded.

    ```{r}
    flights_tzone <- flights_tzone |>
      filter(!is.na(tzone_dest))
    ```

    The tibble is then mutated so the `*_time` variables account for the `tzone`. This can be done using `force_tz`.

    ```{r}
    flights_tzone <- flights_tzone |>
      mutate(
        dep_time = force_tz(dep_time, tzone = tzone_origin),
        sched_dep_time = force_tz(sched_dep_time, tzone = tzone_origin),
        arr_time = force_tz(arr_time, tzone = tzone_dest, roll_dst = "post"),
        sched_arr_time = force_tz(sched_arr_time, tzone = tzone_dest)
      )
    ```

    The test run on `flights_dt2` is now run on the tibble accounting for the time zones, i.e. `flights_tzone`:

    ```{r}
    flights_tzone |>
      filter(!is.na(air_time)) |>
      mutate(
        flight_time = as.numeric(arr_time - dep_time),
        difference = flight_time - air_time
        ) |>
      select(flight_time, air_time, difference) |>
      count(difference > 0) |>
      mutate(prop = n/sum(n))
    ```

    Now for 100% percent of the entries, the `flight_time` is always greater than the `air_time`. The distribution of difference:

    ```{r}
    flights_tzone |>
         filter(!is.na(air_time)) |>
         mutate(
           flight_time = as.numeric(arr_time - dep_time),
           difference = flight_time - air_time
           ) |>
         select(flight_time, air_time, difference) |>
      ggplot(aes(difference)) +
      geom_boxplot()
    ```

4.  How does the average delay time change over the course of a day? Should you use `dep_time` or `sched_dep_time`? Why?

    ```{r}
    flights_dt2 |>
      mutate(day_hour = hour(sched_dep_time)) |>
      summarise(
        avg_dep_delay = mean(dep_delay, na.rm = TRUE),
        .by = day_hour
      ) |>
      ggplot(aes(x = day_hour, y = avg_dep_delay)) +
      geom_point() +
      geom_smooth()
    ```

5.  On what day of the week should you leave if you want to minimize the chance of a delay?

    ```{r}
    #| layout-ncol: 2
    #| fig-width: 4

    flights_dt2 |>
      mutate(week_day = wday(sched_dep_time)) |>
      summarise(
        avg_dep_delay = mean(dep_delay),
        avg_arr_delay = mean(arr_delay),
        .by = week_day
      ) |>
      pivot_longer(
        cols = c(avg_dep_delay, avg_arr_delay),
        names_to = "delay",
        values_to = "mean"
        ) |>
      ggplot(aes(x = week_day, y = mean, fill = delay)) +
      geom_bar(stat = "identity", position = "dodge") +
      labs(
        title = "Average arrival and departure delays",
        y = "Minutes"
      ) +
      scale_fill_discrete(name = NULL, labels = c("Arrival", "Departure"))  +
      scale_x_continuous(
        name = NULL,
        breaks = 1:7,
        labels = c("Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun")
      )

    flights_dt2 |>
      mutate(week_day = wday(sched_dep_time)) |>
      summarise(
        med_dep_delay = median(dep_delay),
        med_arr_delay = median(arr_delay),
        .by = week_day
      ) |>
      pivot_longer(
        cols = c(med_dep_delay, med_arr_delay),
        names_to = "delay",
        values_to = "median"
        ) |>
      ggplot(aes(x = week_day, y = median, fill = delay)) +
      geom_bar(stat = "identity", position = "dodge") +
      labs(
        title = "Median arrival and departure delays",
        y = "Minutes"
      ) +
      scale_fill_discrete(name = NULL, labels = c("Arrival", "Departure"))  +
      scale_x_continuous(
        name = NULL,
        breaks = 1:7,
        labels = c("Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun")
      )
    ```

6.  What makes the distribution of `diamonds$carat` and `flights$sched_dep_time` similar?

    ```{r}
    #| layout-ncol: 2
    #| fig-width: 4
    diamonds |>
      ggplot(aes(x = carat)) +
      geom_freqpoly(binwidth = 0.01)

    flights_dt2 |>
      ggplot(aes(x = minute(sched_dep_time))) +
      geom_freqpoly(binwidth = 1)
    ```

    Both variables `carat` and `sched_dep_time` have a bias towards "nice" number, i.e. multiple of five, and the distributions reflect this bias.

7.  Confirm our hypothesis that the early departures of flights in minutes 20-30 and 50-60 are caused by scheduled flights that leave early. (Hint: create a binary variable that tells you whether or not a flight was delayed.)

    ```{r}
    flights_dt2 |>
      mutate(
        early_flights = dep_delay <= 0,
        dep_interval = case_when(
          minute(sched_dep_time) < 10 ~ "[0 10)",
          minute(sched_dep_time) < 20 ~ "[10 20)",
          minute(sched_dep_time) < 30 ~ "[20 30)",
          minute(sched_dep_time) < 40 ~ "[30 40)",
          minute(sched_dep_time) < 50 ~ "[40 50)",
          minute(sched_dep_time) < 60 ~ "[50 60)",
        ),
        dep_interval = factor(dep_interval,
                              levels = c("[0 10)", "[10 20)", "[20 30)", 
                                         "[30 40)", "[40 50)", "[50 60)")
        )
      ) |>
      summarise(
        prop_early = mean(early_flights),
        .by = c(dep_interval)
      ) |>
      ggplot(aes(x = dep_interval, y = prop_early)) +
      geom_point(size = 2) +
      geom_segment(aes(xend = dep_interval, yend = 0)) +
      labs(
        x = "Departure interval",
        y = "Proportion",
        title = "Earlier or on-time departure proportion per interval"
      )
    ```

## Time Spans {#sec-time-spans}

Next you'll learn about how arithmetic with dates works, including subtraction, addition, and division. Along the way you'll learn about three important classes that represent time spans:

-   **Durations**, which represent an exact number of seconds
-   **Periods**, which represent human units like weeks and months
-   **Intervals**, which represent a starting and ending point.

How do you pick between duration, periods and intervals? As always, pick the simplest data structure that solves your problem. If you only care about physical time, use a duration; if you need to add human times, use a period; if you need to figure out how long a span is in human units, use an interval.

### Durations

In R, when you subtract two dates, you get a difftimes object:

```{r}
# How old is Hadley?
h_age <- today() - ymd("1979-10-14")
h_age
```

A `difftime` class object records a time span of seconds, minutes, hours, days, or weeks. This ambiguity can make difftimes a little painful to work with, so lubridate provides an alternative which always uses seconds: the function `as.duration(h_age)`

```{r}
as.duration(h_age)
```

Durations come with a bunch of convenient constructors:

```{r}
dseconds(15)
dminutes(15)
dhours(c(12, 24))
ddays(0:5)
dweeks(3)
dyears(1)
```

Durations always record the time span in seconds. Larger units are created by converting minutes, hours, days, weeks and years to seconds. Larger units are more problematic. A year uses the "average" number of days in a year, i.e. 365.25. There's no way to convert a month to a duration, because there's just too much variation.

You can add and multiply durations:

```{r}
2 * dyears(1)
dyears(1) + dweeks(12) + dhours(15)
```

You can add and subtract durations to and from days:

```{r}
tomorrow <- today() + ddays(1)
last_year <- today() - dyears(1)
```

Because durations represent an exact number of seconds, sometimes you might get an unexpected result:

```{r}
one_am <- ymd_hms("2026-03-08 01:00:00",
                  tz = "America/New_York")
one_am
one_am + ddays(1)
```

Why is one day after 1am March 8, 2am March 9? If you look carefully at the date, you might also notice that the time zones have changed. March 8 only has 23 hours because it's when DST starts so if we add a full days worth of seconds we end up with a different time.

### Periods

To solve this problem, lubridate provides **periods**. Periods are time spans but don't have a fixed length in seconds, instead they work with human times, like days and months. That allows them to work in a more intuitive way:

```{r}
one_am 
one_am + days(1)
```

Like durations, periods can be created with a number of friendly constructor functions.

```{r}
hours(c(12, 24))
days(7)
months(1:6)
```

You can add and multiply periods:

```{r}
10 * (months(6) + days(1))
days(50) + hours(25) + minutes(2)
```

And of course, add them to dates. Compared to durations, periods are more likely to do what you expect:

```{r}
# A leap year
ymd(2024-01-01) + dyears(1)
ymd(2024-01-01) + years(1)

# Daylight saving time
one_am + ddays(1)
one_am + days(1)
```

Let's use periods to fix an oddity related to our flight dates. Some planes appear to have arrived at their destination *before* they departed from New York City.

```{r}
flights_dt |>
  filter(arr_time < dep_time)
```

These are overnight flights. We used the same date information for both the departure and the arrival times, but these flights arrived on the following day. We can fix this by adding `days(1)` to the arrival time of each overnight flight.

```{r}
flights_dt <- flights_dt |>
  mutate(
    overnight = arr_time < dep_time,
    arr_time = arr_time + days(overnight),
    sched_arr_time = sched_arr_time + days(overnight)
  )
```

Now all of our flights obey the laws of physics:

```{r}
flights_dt |>
  filter(arr_time < dep_time)
```

### Intervals

What does `dyears(1) / ddays(365)` return? It's not quite one, because `dyears()` is defined as the number of seconds per average year, which is 365.25 days.

What does `years(1) / days(1)` return? Well, if the year was 2015 it should return 365, but if it was 2016, it should return 356. There's not quite enough information for lubridate to give a single clear answer. What it does it gives instead an estimate:

```{r}
years(1) / days(1)
```

If you want a more accurate measurement, you'll have to use an **interval**. An interval is a pair of starting and ending date times, or you can think of it as a duration with a starting point.

You can create an interval by writing `start %--% end`:

```{r}
y2023 <- ymd("2023-01-01") %--% ymd("2024-01-01")
y2024 <- ymd("2024-01-01") %--% ymd("2025-01-01")
```

You could then divide it by `days()` to find out how many days fit in the year:

```{r}
y2023 / days(1)
y2024 / days(1)
```

### Exercises

1.  Explain `days(!overnight)` and `days(overnight)` to someone who has just started learning R. What is the key fact you need to know?

    In the `flight_dt` the `overnight` variable is a binary (boolean) variable, containing `TRUE` and `FALSE` values, depending if the arrival time is prior to the departure time or not (i.e. indicating an overnight flight that lands the next day). The `lubridate` constructor function `days()` creates (or parses) a day(s) period object. `TRUE` and `FALSE` are interpreted as 1 and 0 respectively when passed to the function, hence `days(TRUE)` will return a one day period, while `days(FALSE)` a zero days period:

    ```{r}
    days(TRUE)
    days(FALSE)
    ```

2.  Create a vector of dates giving the first day of every month in 2015. Create a vector of dates giving the first day of every month in the current year.

    ```{r}
    # vector of dates giving the first day of every month in 2015
    ymd("2015-01-01") + months(0:11)
    # vector of dates giving the first day of every month in the current year
    ymd(str_flatten(c(year(today()), "01", "01"), collapse = "-")) + months(0:11)
    ```

3.  Write a function that given your birthday (as a date), returns how old you are in years.

    ```{r}
    how_old_are_you <- function(bday) {
      age_in_years <- floor(bday %--% today() / years(1))
      return(age_in_years)
    }

    how_old_are_you(c(ymd("1999-02-23"), ymd("1999-02-24"), ymd("1999-02-25")))
    ```

4.  Why canât `(today() %--% (today() + years(1))) / months(1)` work?

    ```{r}
    today() %--% (today() + years(1)) / months(1)
    ```

## Time zones

Time zones are an enormously complicated topic because of their interaction with geopolitical entities. Fortunately, we don't need to dig into all the details as they're not all important for data analysis, but there are a few challenges we'll need to tackles head on.

The first challenge is that everyday names of the time zones tend to be ambiguous. For example, if you're American, you're probably familiar with EST, or Eastern Standard Time. However, both Australian and Canada also have EST. To avoid confusion, R uses the international standard IANA time zones. These use a consistent naming scheme `{area}/{location}`, typically in the form of `{continent}/{city}` or `{ocean}/{city}`. Examples include `"America/New_York"`, `"Europe/Paris"` and `"Pacific/Aukland"`.

You might wonder why the time zone uses a city, when typically you think of time zones as associated with a country or region within a country. This is because the IANA database has to record decades worth of time zone rules. Over the course of decades, countries change names (or break apart) fairly frequent, but city names tend to stay the same. Another problem is that the name needs to reflect not only the current behavior, but also the complete history. For example there are time zones for both `"America/New_York"` and `"America/Detroit"`. These cities both currently use Eastern Standard Time but in 1969-1972 Michigan (the state in which Detroit is located) did not follow DST, so it needs a different name. It's worth reading the raw time zone database (available at <https://www.iana.org/time-zones>) just to read some of these stories.

You can find out what R thinks your current time zone is with `Sys.timezone()`:

```{r}
Sys.timezone()
```

(If R doesn't know, you'll get an `NA`.)

And see the complete list of all times zone with `OlsonNames()`:

```{r}
length(OlsonNames())
head(OlsonNames())
```

In R, the time zone is an attribute of the date-time that only controls printing. For example, these three objects represent the same instant in time:

```{r}
x1 <- ymd_hms("2024-06-01 12:00:00", tz = "America/New_York")

x2 <- ymd_hms("2024-06-01 18:00:00", tz = "Europe/Copenhagen")

x3 <- ymd_hms("2024-06-02 04:00:00", tz = "Pacific/Auckland")
```

You can verify that they're the same time using subtraction:

```{r}
x1 - x2
x1 - x3
```

Unless otherwise specified, `lubridate` always uses UTC. UTC (Coordinated Universal Time) is the standard time zone used by the scientific community and is roughly equivalent to GMT (Greenwich Mean Time). It does not have DST, which makes a convenient representation for computation. Operations that combine date-time, like `c()`, will often drop the time zone. In that case, the date-time will display in the timezone of the first element:

```{r}
x4 <- c(x1, x2, x3)
x4
```

You can change the time zone in two ways:

-   Keep the instant in time the same, and change how it's displayed. Use this when the instant is correct but you want a more natural display:

    ```{r}
    x4a <- with_tz(x4, tzone = "Australia/Lord_Howe")
    x4a
    x4a-x4
    ```

-   Change the underlying instant in time. Use this when you have an instant that has been labeled with the incorrect time zone, and you need to fix it.

    ```{r}
    x4b <- force_tz(x4, tzone = "Australia/Lord_Howe")
    x4b
    x4b-x4
    ```

## Summary

This chapter has introduced you to the tools that lubridate provides to help you work with date-time data. Working with dates and times can seem harder than necessary, but hopefully this chapter has helped you see why -- date-times are more complex that they seem at first glance, and handling every possible situation adds complexity. Even if your data never crosses a day light savings boundary or involves a leap year, the functions need to be able to handle it.
